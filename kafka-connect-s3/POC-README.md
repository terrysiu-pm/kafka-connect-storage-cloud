# Kafka Connect S3 Extension

Sample extension of [Kafka Connect S3 Sink Connector](https://github.com/confluentinc/kafka-connect-storage-cloud/tree/master/kafka-connect-s3).

> NOTE: This sample is an exploratory exercise in extending an existing connector and debugging a connector locally

This extension adds:
* [ProtobufFormat](./src/main/java/io/confluent/connect/s3/format/protobuf/ProtobufFormat.java)
  * Very similar to the ByteArrayFormat, with the exception that the writer provider used by the ProtobufFormat prepends
    the length of each record to the byte array. ByteArrayFormat appends newline to each record, which does not work
    with protobuf encoding
* [PMCustomerPartitioner](./src/main/java/io/confluent/connect/s3/partitioner/PMCustomPartitioner.java)
  * Extends TimeBasedPartitioner
  * Uses new S3 sink connector config, `header.partition.fields`, which defines a comma, delimited list of header fields
    to form partition folders that are prepended to the folders generated by the TimeBasedPartitioner. This use-case
    is applied for scenarios where events in a single topic need to be further partitioned by specific header fields,
    similar to header-based, event routing

## Pre-requisites
* JDK 11
* Maven 3 (I have 3.8.5)
* Local install of Kafka 3.2.0. [Download](https://kafka.apache.org/downloads). Make sure the `bin` directory is a part of your `PATH` variable

## Build
This fork is based off of the [v10.0.9 release](https://github.com/confluentinc/kafka-connect-storage-cloud/tree/v10.0.9).

There is a small change to the [parent pom.xml](../pom.xml) to use a hard-code repository value instead of using
property substitution. For whatever reason, the property substitution was not working for me in my local build.

Execute `mvn package` to build the artifacts.

## Configuration
### Connect Properties
> NOTE: The Connect properties was only tested against a cluster using PLAIN SASL mechanism in Confluent Cloud.
> Other managed Kafka offerings, such as Redpanda, will have different authentication requirements

We will run Kafka Connect in standalone mode to enable debugging of the S3 sink connector. A template for the Kafka
Connect properties can be found [here](poc-config/connect-standalone.properties). You'll need to substitute in the
values indicated in the template. Properties of note:
* `consumer.*`
  * Required for sink connectors. See [Kafka Connect Security](https://docs.confluent.io/platform/current/kafka/authentication_sasl/authentication_sasl_gssapi.html#kconnect-long)
* `header.converter`
  * Must use a StringConverter. Otherwise, if not specified, the header value type will be inferred. For the scenario
    where a header value sent is the string `0000`, Kafka Connect will receive a header value of the integer value `0`.
    The StringConverter preserves the original `0000` value
* `plugin.path`
  * Once you get a full build, you'll need to update this value to point to `<git clone location of project>/kafka-connect-s3/target/components/packages/confluentinc-kafka-connect-s3-10.0.9`
    which contains all the libs needed to run the sink connector

### Sink Connector Properties
> NOTE: The sink connector properties was only tested against a cluster using PLAIN SASL mechanism in Confluent Cloud.
> Other managed Kafka offerings, such as Redpanda, will have different authentication requirements

[Sample sink connector properties](poc-config/s3-sink-connector.properties). Substitute in the values indicated
in the template. Properties of note:
* `header.partition.fields`
  * Compatible with event headers generated by [evgen-lib](https://github.com/ProbablyMonsters/kraken/tree/main/poc/streaming/evgen-lib).
    Comma-delimited list of header fields to form extra partition folders prepended to the ones formed by the TimeBasedPartitioner

## Execute
Once you have constructed the property files from the templates, we can launch a standalone Kafka Connect using the
customized S3 Sink connector. If you wish to debug the connector, execute:

```shell
env KAFKA_DEBUG=y DEBUG_SUSPEND_FLAG=y connect-standalone.sh <path to your Connect properties> <path to your S3 sink connector properties>
```

The standalone Connect will listen on port `5005` and wait for you to connect with a remote JVM Debug session which you
can launch with these arguments:

```text
-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:5005
```

Use [evgen-lib](https://github.com/ProbablyMonsters/kraken/tree/main/poc/streaming/evgen-lib) to send 5-10 sample
protobuf events to the configured topic for your S3 sink connector. If everything works, you should see in your bucket,
the following example folders:

```text
protobuf-format/test-connect/organization_id=A72A/title_id=0000/year=2022/month=07/day=01/hour=16/
```

Happy debugging!!!

## TODO
* Run Kafka Connect via Docker
